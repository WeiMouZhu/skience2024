{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <IMG SRC=\"logoeost.png\" WIDTH=100 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Classification of Seismic Sources - Random Forest Classifier\n",
    "\n",
    "\n",
    "Based on, and with the courtesy of, the \"*IA in geosciences\" practical by C. Hibert / 28 January 2020*.\n",
    "\n",
    "Adapted for the Skience2024 workshop (generalisation to N different classes & translation) by Thomas Lecocq.\n",
    "\n",
    "---------\n",
    "\n",
    "In this tutorial you will see how to implement a machine learning algorithm for a discrimination/classification problem using the Python function library `sickit-learn`. This function library is very comprehensive and one of the most widely used in the world for everything to do with Machine Learning. \n",
    "\n",
    "\n",
    "You will be working on seismological data, with the aim of achieving the best rate of correct identification between any number of source: signals generated by volcano-tectonic earthquakes, other type of volcano-generated signals, as well as noise samples. Having an algorithm that can make this discrimination on continuous data will make it possible to reconstruct chronicles of events on a volcano. These chronicles will potentially provide a better understanding of the volcano dynamics.\n",
    "\n",
    "The dataset we computed includes a (very) small number of labelled \"events\" recorded by the a temporary deployment on Mount Merapi, Indonesa.\n",
    "\n",
    "## Train & Classify!\n",
    "\n",
    "The signals have already been transformed into a set of 58 attributes using the previous notebook.\n",
    "\n",
    "The code block below loads the libraries that will be needed in this tutorial and loads the different data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from ComputeAttributesV_MAT import get_attribute_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "## Exemple ##\n",
    "\n",
    "#classname=('IceQ','EQ')\n",
    "#plt.figure()\n",
    "#plot_confusion_matrix(metrics.confusion_matrix(Classes, Y_pred),classes=classname)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station = \"GRW0\"\n",
    "channel = \"BHZ\"\n",
    "\n",
    "classname=('VTB','MP', \"gugu_short\", \"gugu_long\", \"NN\", \"ND\")\n",
    "arrays = []\n",
    "for c in classname:\n",
    "    fn = os.path.join(\"attributes\", \"%s.%s\"%(station, channel), \"%s.npy\"%c)\n",
    "    data = np.load(fn)\n",
    "    print(c, \":\", len(data), \"items\")\n",
    "    arrays.append(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by preparing the data:\n",
    "  - Determine the number of events per class\n",
    "  - Clean up the data: eliminate events for which outliers have been calculated (NaN and Inf; functions `np.isinf` and `np.isnan` for example)\n",
    "  - Create variables that will randomly select a number _n_ of events from our training dataset (you can use the `np.random.randint` function).\n",
    "  - From these variables, create the matrix containing the attributes for the training, and the associated matrix containing the corresponding classes (function `np.concatenate`). For the classes we need to associate integers with each of them. We'll arbitrarily assign a class number to each of the N event types. We'll start with a training set containing 5 events from each class.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_arrays(arrays):\n",
    "    processed_arrays = []\n",
    "    \n",
    "    for array in arrays:\n",
    "        array = array[~np.isinf(array).any(axis=1)]\n",
    "        array = array[~np.isnan(array).any(axis=1)]\n",
    "        array = array[:, 0:58]\n",
    "        processed_arrays.append(array)\n",
    "        \n",
    "    return processed_arrays\n",
    "\n",
    "def generate_train_data(NbrofEvent, processed_arrays):\n",
    "    # processed_arrays = process_arrays(arrays)\n",
    "    \n",
    "    rand_ids = [np.random.choice(np.arange(len(arr)-1), size=NbrofEvent, replace=False).astype(np.int32) for arr in processed_arrays]\n",
    "    \n",
    "    train_attrib = np.concatenate([arr[rand_id, :] for arr, rand_id in zip(processed_arrays, rand_ids)], 0)\n",
    "    \n",
    "    # train_class = np.concatenate([np.zeros((len(rand_id), 1)) if i == 0 else np.ones((len(rand_id), 1)) for i, rand_id in enumerate(rand_ids)], 0)\n",
    "    train_class = np.concatenate([np.full((len(rand_id), 1), i) for i, rand_id in enumerate(rand_ids)], 0)\n",
    "    return train_attrib, train_class, rand_ids\n",
    "\n",
    "# Example usage with 2 arrays ATTEQ and ATTIceQ\n",
    "processed_arrays = process_arrays(arrays)\n",
    "\n",
    "AttributesVal = np.concatenate([arr[1:] for arr in processed_arrays], 0)\n",
    "Classes = np.concatenate([np.full((len(arr)-1, 1), i) for i, arr in enumerate(processed_arrays)], 0)\n",
    "\n",
    "NbrofEvent = 5\n",
    "Train_Attrib, Train_Class, ids = generate_train_data(NbrofEvent, processed_arrays)\n",
    "\n",
    "Test_AttributesValRep = np.concatenate([np.delete(arr[1:], rand, 0) for arr, rand in zip(processed_arrays, ids)], 0)\n",
    "Test_ClassesRep = np.concatenate([np.delete(np.full((len(arr)-1, 1), i), rand, 0) for i, (arr, rand) in enumerate(zip(processed_arrays, ids))], 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to create and train the classifier:\n",
    "  - Create a Random Forest classifier with 500 trees and store this model in a variable we'll call `clf` (see [RandomForestClassifier](https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/modules/generated/sklearn.ensemble.RandomForestClassifier.html)).\n",
    "  - Train the classifier with the training dataset you created using the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(500)\n",
    "clf.fit(Train_Attrib,Train_Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Model identification and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to using the trained model to identify the events in our dataset:\n",
    "\n",
    "- Predict the class of all elements in the dataset using the `predict` method. Determine the accuracy of the classification using the `metrics.precision_score` function.\n",
    "- Represent the results as a confusion matrix.\n",
    "- Predict the class only of events that were not used for training, determine the precision and represent the results in the form of a confusion matrix (tip: `np.delete`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=clf.predict_proba(AttributesVal)\n",
    "Y_pred=clf.predict(AttributesVal)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(metrics.confusion_matrix(Classes, Y_pred),classes=classname)\n",
    "plt.show()\n",
    "\n",
    "print(metrics.precision_score(Classes, Y_pred, average=\"weighted\"))\n",
    "\n",
    "                                                \n",
    "Y_PredNoRep=clf.predict(Test_AttributesValRep)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(metrics.confusion_matrix(Test_ClassesRep, Y_PredNoRep),classes=classname)\n",
    "plt.show()\n",
    "\n",
    "print(metrics.precision_score(Test_ClassesRep, Y_PredNoRep, average=\"weighted\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Importance of attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have used 58 attributes to describe the signals (full list at the end of this tutorial). For processing hundreds of thousands of events, the most time-consuming step is the transformation of the signals into attributes (spectrum calculation, etc.). The _Random Forest_ algorithm is used to determine the importance of attributes in the discrimination process, by giving each attribute a score:\n",
    "  \n",
    "  - Determine the importance of each attribute in the identification process you carried out in __C.__. (method `feature_importances_`). You can represent these values (e.g. `plt.plot`, `plt.stem`, etc.).\n",
    "  - Repeat the training and identification process, using only the 10 attributes with the highest score.\n",
    "  - What do you deduce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,12))\n",
    "plt.stem(clf.feature_importances_, orientation=\"horizontal\")\n",
    "# plt.xticks(range(0,58))\n",
    "plt.yticks(range(58), get_attribute_names().values())\n",
    "plt.margins(0.01)\n",
    "plt.show()\n",
    "\n",
    "Indices_Att_trie=np.argsort(clf.feature_importances_)\n",
    "#print(Indices_Att_trie)\n",
    "\n",
    "Indices_10best=Indices_Att_trie[-10::]\n",
    "\n",
    "Train_Attrib10=Train_Attrib[:,Indices_10best]\n",
    "Train_Class10=Train_Class\n",
    "\n",
    "\n",
    "clf10=RandomForestClassifier(500)\n",
    "clf10.fit(Train_Attrib10,Train_Class10)\n",
    "\n",
    "AttributesVal10=Test_AttributesValRep[:,Indices_10best]\n",
    "# Classes=np.concatenate((np.zeros((len(ATTIceQ)-1,1)),np.ones((len(ATTEQ)-1,1))),0)\n",
    "\n",
    "pred=clf10.predict_proba(AttributesVal10)\n",
    "Y_pred10=clf10.predict(AttributesVal10)\n",
    "\n",
    "print(metrics.precision_score(Test_ClassesRep, Y_pred10, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the random forest for reuse\n",
    "import joblib\n",
    "# save\n",
    "joblib.dump(clf, \"./random_forest.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
